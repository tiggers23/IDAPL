# Rethinking the Effect of Uninformative Class Name in Prompt Learning
This repo contains the code for the IDAPL research project, which focuses on investigating the effect of uninformative classname in prompt Learning. This code is based on the CoOp[缺引用] implementation.
# How to install
This code is built on top of the toolbox Dassl.pytorch【缺引用】 so you need to install the dassl environment first. Simply follow the instructions described here to install dassl as well as PyTorch. After that, run pip install -r requirements.txt under CoOp/ to install a few more packages required by CLIP (this should be done when dassl is activated). Then, you are ready to go.
Follow DATASETS.md【缺引用】 to install the datasets. And if you want to use the description generated by GPT to standardize the direction of prompt learning, please thank and download the data support provided by CLIP-A-Self【缺引用】.
# Few-shot setting on 11 datasets.
## How to run!
#We have chosen hyperparameters 0.1 and 5.0 for the weights of learnable class vectors and classification loss weights, but they are generally robust in most cases. You can choose based on your downstream tasks
```Bash
bash scripts/coop_idapl/main.sh ${DATSET_NAME} ${CFG} ${SHOTS} ${N_CTX} ${ASSOCIATIVE_LEARNING} ${SCORE_LC} ${SCORE_CLF}
```
For Example:
```Bash
bash scripts/coop_idapl/main.sh standford_cars vit_b16_ep50_ctxv1 16 16 True 0.1 0.5
```
## How to test!
When you migrate to a novel class, please set ASSOCIATIVE_LEARNING False, and the learnable vectors are only used for the base class!
```Bash
bash scripts/coop_idapl/eval.sh ${DATSET_NAME} ${CFG} ${SHOTS} ${N_CTX} ${ASSOCIATIVE_LEARNING} ${SCORE_LC} ${SCORE_CLF}
```
This means that when you load the model of the previous run example, you do not use the learnable vectors of categories
For Example:
```Bash
bash scripts/coop_idapl/eval.sh standford_cars vit_b16_ep50_ctxv1 16 16 False 0.1 0.5
```
# Citation
If you use this code in your research, please kindly cite the following papers
【缺引用】
